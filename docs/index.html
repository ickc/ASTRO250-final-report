<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Kolen Cheung">
  <meta name="keywords" content="ASTRO250, Parallel Computing, POLARBEAR, CMB, final project">
  <title>ASTRO 250 Final Project</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ickc/markdown-latex-css/css/common.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/ickc/markdown-latex-css/fonts/fonts.min.css">
  <p></p>
	<script type="text/javascript">
	  window.MathJax = {
	    TeX: {
	      equationNumbers: { autoNumber: "AMS" },
	      extensions: ["cancel.js","[Contrib]/physics/physics.js","[Contrib]/siunitx/siunitx.js"]
	    }
	  };
	</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">ASTRO 250 Final Project</h1>
<p class="subtitle">Spring 2017, University of California, Berkeley</p>
<p class="author">Kolen Cheung</p>
<p class="date">May  5, 2017</p>
</header>
<h1 id="introduction"><span class="header-section-number">1</span> Introduction</h1>
<p>Physicists always push the boundary of our understanding of the most fundamental aspects of the Universe. Some of the fundamental questions we can ask are what constitute the Universe; how gravity plays a role in quantum Physics; why neutrino mass are non-zero, how much mass do they have, and how many numbers of them; and if the current understanding of the Universe through <span class="math inline">\(\Lambda\)</span>-CDM model is correct.</p>
<p>Many of such questions can only be answered when we probed at higher and higher energy scales. For example, the highest energy scale we can achieve artificially in the state-of-the-art LHC is about <span class="math inline">\(\sim \SI{10}{\TeV}\)</span>, or <span class="math inline">\(\SI{10e13}{\eV}\)</span>. But we can do only so much experimentally because of the limit of the size of the equipment we can build, and it is unlikely for the foreseeable future to create energy scale as high as the GUT scale at <span class="math inline">\(\sim \SI{10e16}{\GeV}\)</span>, or <span class="math inline">\(\SI{10e25}{\eV}\)</span>, which will be important for Quantum Gravity.</p>
<p>So instead of relying on human-built machine, one can measure the primordial signals created by the Universe itself. And the oldest possible such signal that is observable is the Cosmic Microwave Background (CMB) Radiation. It is the first light of the Universe when it was <span class="math inline">\(\sim 400000\)</span> years young, and everything happened between now and then are imprinted in this signal. Some of the information we can extracts includes gravitational wave at GUT scale (by B-mode analysis on the CMB), dark matter, neutrino mass, falsification of <span class="math inline">\(\Lambda\)</span>-CDM model, etc.</p>
<p>POLARBEAR is one of the pioneer group on the measurement of CMB polarization in University of California, Berkeley. One of the major result by POLARBEAR in 2014 is<span class="citation" data-cites="Collaboration:2014eg">(Collaboration et al. 2014)</span>:</p>
<blockquote>
<p>the hypothesis of no B-mode polarization power from gravitational lensing is rejected at <span class="math inline">\(97.2\%\)</span> confidence</p>
</blockquote>
<p><embed src="media/CMB-B-mode.png" /> </p>
<p>But in order to extract more information from the huge amount of data collected, we need to do a high-<span class="math inline">\(l\)</span> analysis, which resolves to the native resolution of the telescope data. Previous analysis is downsampled because it was computationally prohibitive.</p>
<p>Below we will describe the computational requirement and lay out a plan to lower it to an affordable level of computational cost.</p>
<h1 id="algorithm"><span class="header-section-number">2</span> Algorithm</h1>
<p>The majority of the algorithms involved are custom-deigned for the experiments, which relies on existing libraries such as Numpy. On NERSC, Intel’s Distribution of Python (with Anaconda) is used, and since libraries such as Numpy are built with OpenMP and SIMD support, parallelization is automatic and free whenever those libraries are used.</p>
<p>Among the custom codes, there are 2 major bottleneck:</p>
<ol>
<li>pseudo-power spectrum (computational intensive),</li>
<li>weighted averages and statistics (huge intermediate write on scratch disks).</li>
</ol>
<p>The followings layout the plan to solve these 2 problems.</p>
<h1 id="parallelization-and-scalability"><span class="header-section-number">3</span> Parallelization and Scalability</h1>
<p>There are many Python/C codes that are custom built in POLARBEAR, and unfortunately are all in serial due to historical reasons.</p>
<p>Part of this project is to study the viability of rewriting the code in Cython instead. Cython is a superset of Python which can be translated into C/C++ code and compiled. Because of that, we can continue to write Pythonically when performance is not critical, but in C-styled Cython code if performance is critical. Moreover, SIMD optimization and OpenMP are also supported in Cython. Although there are limitations on the OpenMP and SIMD features that can be used, these limitation turns out not to be a problem in our study. It is because our analysis involve telescope data, where each hour-long observation are independent, and each channels within an observation are also independent for most of the analysis, parallelizing the code within the Cython language is doable in all the performance critical loops in my study. There is a vectorization that can only be achieved by <code>#pragma ivdep</code> in C and unfortunately not supported in Cython, but is not performance critical. In fact, OpenMP parallelization is so easy to use in Cython (as in C) in our study, that most effort of OpenMP parallelization is not spent on whether it is parallelizable but whether if it is worth the OpenMP overhead because the algorithm might be I/O bounded.</p>
<h1 id="optimizations-and-improvements"><span class="header-section-number">4</span> Optimizations and Improvements</h1>
<p>Here we will focus on the 2 limiting factors that makes the current analysis very expensive, and a proposal on solving these 2 problems. And lastly, the time and space cost is estimated.</p>
<h2 id="computational-hotspot"><span class="header-section-number">4.1</span> Computational Hotspot</h2>
<p>In the current pipeline, the computation cost is estimated to be:</p>
<ul>
<li><span class="math inline">\(700\)</span> simulation and <span class="math inline">\(1\)</span> real data</li>
<li><span class="math inline">\(14\)</span> splits for null test</li>
<li><span class="math inline">\(90 \min\)</span> per spectra</li>
<li><span class="math inline">\(3\)</span> spectra each</li>
<li>Cori Haswell charge factor: <span class="math inline">\(80\)</span> hours per node</li>
<li><span class="math inline">\(16\)</span> processes per node due to RAM limit</li>
</ul>
<p>So the total amount of NERSC hours needed are <span class="math inline">\(\sim 220,000\)</span> NERSC hours.</p>
<p>One of the most computational intensive part of the pipeline is the apodization mask in the pseudo-power spectrum, which basically smoothen the transition on the boundary of the map. The old algorithm is <span class="math inline">\(O(n^3)\)</span> and relies on Numpy’s functions and because of the limited choices of functions, the algorithm is going through unnecessary steps and is very slow. I proposed an algorithm that is <span class="math inline">\(O(n)\)</span>, based on a boundary tracing algorithm. Although this algorithm is very hard to parallelize, it scales as <span class="math inline">\(O(n)\)</span> which shows its advantage when we are doing a full resolution analysis as <span class="math inline">\(n\)</span> becomes very large.</p>
<p>In addition, I expect speed-up from Cythonization including SIMD vectorization and OpenMP multi-threading. From OpenMP, I estimated we would get about a 4-fold improvement, because in our current pipeline we only start 16 processes on Cori’s Haswell nodes due to RAM limit and therefore wasting half numbers of idle cores, and since our pipeline are mostly IO bounded, I expect another 2-fold from multi-threading. From SIMD, I conservatively estimate a factor of 2 improvement, because of vectorization efficiency, some loops cannot be vectorized, and some are already done in the Numpy functions. I expect a 4-fold improvement from Cythonization based on preliminary test due to the inefficiency of using Numpy in Python. And conservatively, we divide it by a factor of 2 to account for possible overestimation. So the total speed-up factor is estimated to be 16.</p>
<p>So to conclude, if all optimization scheme laid out above is used, the total number of NERSC hours needed for this analysis would be <span class="math inline">\(\sim 14,000\)</span> hours.</p>
<p>However, Cythonization is a lengthy process. To account for the fact that we want to perform this analysis as soon as possible and therefore we might not be able to fully cythonize our application, we conservatively estimated we need <span class="math inline">\(100,000\)</span> NERSC hours, including profiling and debugging cost.</p>
<h2 id="disk-space-hotspot"><span class="header-section-number">4.2</span> Disk Space Hotspot</h2>
<p>In the current pipeline, a lot of intermediate files are written. In the 1st stage of the pipeline, each of <span class="math inline">\(4237\)</span> files will have one output, and then a 2nd stage process will load them to co-add them together (calculate a weighted average as well as statistics). So the total intermediate disk space required is:</p>
<ul>
<li>On average <span class="math inline">\(12.3\)</span> MB per observation</li>
<li><span class="math inline">\(4237\)</span> observations</li>
<li>“null factor” — the extra disk space required for null test: <span class="math inline">\(6.67 \pm 0.01\)</span></li>
<li>700 simulation and 1 real data</li>
</ul>
<p>with a total of <span class="math inline">\(244\)</span> TB.</p>
<p>In ideal situation, one would use an MPI reduce to avoid all the intermediate files. However, the legacy code base is too spaghetti-like for such kind of refactoring. My proposal is for each independent process, rather then writing a new output files, it will open the previous one, co-add, and then close the files. So the number of intermediate files no longer proportional to the total number of input <span class="math inline">\(4237\)</span>, but the total number of process, which is much less:</p>
<ul>
<li>original size: <span class="math inline">\(244\)</span> TB</li>
<li>Intermediate disk size limit: <span class="math inline">\(20\)</span> TB from Cori’s scratch</li>
<li><span class="math inline">\(4237\)</span> inputs</li>
<li><span class="math inline">\(16\)</span> process per node</li>
</ul>
<p>So we can start at most <span class="math inline">\(21\)</span> nodes at a time because of the <span class="math inline">\(20\)</span> TB limit from Cori. And from the above computational time estimate, the wall clock time required will be <span class="math inline">\(\sim 8\)</span> hours in the best case senario, and <span class="math inline">\(57\)</span> hours in the conservative estimate. Since the maximum wall-clock time that can be requested is <span class="math inline">\(36\)</span> hours when <span class="math inline">\(21\)</span> nodes is used, the jobs might be needed to be split in <span class="math inline">\(2\)</span>.</p>
<p>Because of the huge amount of IO, Cori’s burst buffer seems to be best fit for this task.</p>
<h1 id="conclusion"><span class="header-section-number">5</span> Conclusion</h1>
<p>To conclude, in order to carry out high-<span class="math inline">\(l\)</span> analysis for POLARBEAR’s 2014-2016 data, it is estimated that we need around <span class="math inline">\(100,000\)</span> NERSC hours, and as much burst buffer spaces as possible, ideally at <span class="math inline">\(20\)</span> TB. If this is impossible, Cori’s standard scratch space will be used.</p>
<h1 id="reference" class="unnumbered">Reference</h1>
<div id="refs" class="references">
<div id="ref-Collaboration:2014eg">
<p>Collaboration TP, Ade PAR, Akiba Y et al (2014) A Measurement of the Cosmic Microwave Background B-Mode Polarization Power Spectrum at Sub-Degree Scales with POLARBEAR. arXivorg 171.</p>
</div>
</div>
</body>
</html>
